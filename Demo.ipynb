{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AppsFiles\\Anaconda3\\envs\\llama\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import docx\n",
    "import os\n",
    "import numpy as np\n",
    "import re\n",
    "from sentence_transformers import SentenceTransformer, util, CrossEncoder\n",
    "from rank_bm25 import BM25Okapi\n",
    "\n",
    "os.environ['TRANSFORMERS_OFFLINE'] = '1'\n",
    "\n",
    "\n",
    "def split_doc_by_heading_with_title(doc):\n",
    "    sections = []\n",
    "    current_section = None\n",
    "    for para in doc.paragraphs:\n",
    "        text = para.text.strip()\n",
    "        if not text:\n",
    "            continue \n",
    "        # judge whether it is a title \n",
    "        if para.style.name.lower().startswith(\"heading\"):\n",
    "            if current_section is not None:\n",
    "                sections.append(current_section)\n",
    "            current_section = {\"title\": text, \"content\": \"\"}\n",
    "        else:\n",
    "            if current_section is None:\n",
    "                # if without title, then create a new section\n",
    "                current_section = {\"title\": \"Intro\", \"content\": \"\"}\n",
    "            # add in contents\n",
    "            if current_section[\"content\"]:\n",
    "                current_section[\"content\"] += \"\\n\" + text\n",
    "            else:\n",
    "                current_section[\"content\"] = text\n",
    "    if current_section is not None:\n",
    "        sections.append(current_section)\n",
    "    return sections\n",
    "\n",
    "doc = docx.Document(\"Advising FAQ (12-19-24 Update).docx\")\n",
    "sections_with_title = split_doc_by_heading_with_title(doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def merge_sections_by_title(sections, model, threshold=0.8):\n",
    "    \"\"\"\n",
    "    concatenate each section in the list (including title and content),\n",
    "    If the titles' cosine similarity is beyond the threshold, then combine then (title and content),\n",
    "    and add title and content together into full_text to save.\n",
    "    \"\"\"\n",
    "    if not sections:\n",
    "        return []\n",
    "    merged = []\n",
    "    used = [False] * len(sections)\n",
    "    titles = [sec[\"title\"] for sec in sections]\n",
    "    # contents = [sec[\"content\"] for sec in sections]\n",
    "    # Calculate all the embeddings of the titles\n",
    "    title_embeddings = model.encode(titles, convert_to_tensor=True, show_progress_bar=False)\n",
    "    # content_embeddings = model.encode(contents, convert_to_tensor=True, show_progress_bar=False)\n",
    "    for i in range(len(sections)):\n",
    "        if used[i]:\n",
    "            continue\n",
    "        merged_title = sections[i][\"title\"]\n",
    "        merged_content = sections[i][\"content\"]\n",
    "        used[i] = True\n",
    "        for j in range(i+1, len(sections)):\n",
    "            if used[j]:\n",
    "                continue\n",
    "            # title_score = util.cos_sim(title_embeddings[i], title_embeddings[j]).item()\n",
    "            # content_score = util.cos_sim(content_embeddings[i], content_embeddings[j]).item()\n",
    "            # sim = 0.5 * title_score + 0.5 * content_score\n",
    "            sim = util.cos_sim(title_embeddings[i], title_embeddings[j]).item()\n",
    "            if sim >= threshold:\n",
    "                # merged_title += \"\\n\" + sections[j][\"title\"]\n",
    "                merged_content += \"\\n\" + sections[j][\"content\"]\n",
    "                used[j] = True\n",
    "        # concatenate title and content into full_text\n",
    "        full_text = merged_title + \"\\n\" + merged_content\n",
    "        merged.append({\"title\": merged_title, \"content\": merged_content, \"full_text\": full_text})\n",
    "    return merged\n",
    "\n",
    "# Calculate cosine similarity\n",
    "embed_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "merged_sections = merge_sections_by_title(sections_with_title, embed_model, threshold=0.6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches:   0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:02<00:00,  1.26it/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "full_text = [sec['full_text'] for sec in merged_sections]\n",
    "# construct BM25 index for sections以合并后的 section 内容建立 BM25 索引\n",
    "tokenized = [tokenize(text) for text in full_text]\n",
    "bm25 = BM25Okapi(tokenized)\n",
    "\n",
    "\n",
    "# generate the embeddings for sections\n",
    "sections_embeddings = embed_model.encode(full_text, convert_to_tensor=True, show_progress_bar=True)\n",
    "sections_embeddings_norm = util.normalize_embeddings(sections_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 3/3 [00:00<00:00, 27.63it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# define the query\n",
    "query = input(\"Please input your query: \")\n",
    "\n",
    "# generate the embedding of the query\n",
    "query_embedding = embed_model.encode([query], convert_to_tensor=True)\n",
    "query_embedding_norm = util.normalize_embeddings(query_embedding)\n",
    "# calculate cosine similarity\n",
    "sections_cosine_scores = util.cos_sim(query_embedding_norm, sections_embeddings_norm)[0].cpu().numpy()\n",
    "\n",
    "# calculate the BM25 score\n",
    "query_tokens = tokenize(query)\n",
    "bm25_scores = bm25.get_scores(query_tokens)\n",
    "if np.max(bm25_scores) > 0:\n",
    "    bm25_scores_norm = bm25_scores / np.max(bm25_scores)\n",
    "else:\n",
    "    bm25_scores_norm = bm25_scores\n",
    "\n",
    "# mix the two scores up\n",
    "sections_score = 0.2 * bm25_scores_norm + 0.8 * sections_cosine_scores\n",
    "sections_weight = 0.5  # weight of section\n",
    "\n",
    "# calculate cosine similarity of section titles and query\n",
    "titles = [sec[\"title\"] for sec in merged_sections]\n",
    "title_embeddings = embed_model.encode(titles, convert_to_tensor=True, show_progress_bar=True)\n",
    "query_title_embedding = embed_model.encode([query], convert_to_tensor=True)\n",
    "title_cosine_scores = util.cos_sim(query_title_embedding, title_embeddings)[0].cpu().numpy()\n",
    "title_weight = 0.5  # weight of the title\n",
    "\n",
    "# Total score\n",
    "overall_scores = sections_weight * sections_score + title_weight * title_cosine_scores\n",
    "\n",
    "\n",
    "\n",
    "# rank by decreasing\n",
    "candidate_indices = np.argsort(-overall_scores)\n",
    "\n",
    "# select top-10\n",
    "top_n = 10\n",
    "top_candidate_indices = candidate_indices[:top_n]\n",
    "candidate_texts = []\n",
    "for idx in top_candidate_indices:\n",
    "    # construct text (title and content)\n",
    "    candidate_texts.append(\"Title: \" + merged_sections[idx][\"title\"] + \"\\nContent: \" + merged_sections[idx][\"content\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cross encoder\n",
    "cross_encoder = CrossEncoder('ms-marco-MiniLM-L12-v2')\n",
    "cross_input = [[query, text] for text in candidate_texts]\n",
    "cross_scores = cross_encoder.predict(cross_input)\n",
    "rerank_order = np.argsort(-cross_scores)\n",
    "\n",
    "k = 5\n",
    "top_reranked_text = \"\\n\".join([candidate_texts[i] for i in rerank_order[:k]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(query, top_reranked_text):\n",
    "    prompt = (\n",
    "        f\"Background Information (ordered by importance, from most important to least important):\\n{top_reranked_text}\\n\\n\"\n",
    "        f\"Question:{query}\\n\\n\"\n",
    "        \"Using all the information above, please generate a clear, comprehensive, and precise answer.\\\n",
    "        \\nAnswer:\"\n",
    "    )\n",
    "    return prompt\n",
    "\n",
    "# enhanced_prompt = construct_prompt(query, top_reranked_text)\n",
    "# print(enhanced_prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enhanced_prompt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mollama\u001b[39;00m\n\u001b[0;32m      3\u001b[0m response \u001b[38;5;241m=\u001b[39m ollama\u001b[38;5;241m.\u001b[39mchat(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mllama3.2\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m----> 4\u001b[0m                        messages\u001b[38;5;241m=\u001b[39m[{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43menhanced_prompt\u001b[49m}])\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmessage\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mNameError\u001b[0m: name 'enhanced_prompt' is not defined"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=\"llama3.2\", \n",
    "                       messages=[{\"role\": \"user\", \"content\": enhanced_prompt}])\n",
    "print(response['message']['content'])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
